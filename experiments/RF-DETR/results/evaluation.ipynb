{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32259b98",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import json\n",
        "from animaloc.eval.metrics import PointsMetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a7702d6",
      "metadata": {},
      "source": [
        "# Evaluate From CSV\n",
        "\n",
        "This notebook computes HerdNet metrics from CSV files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3348fc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "detections_csv = Path(\"detections (1).csv\")  # Generated by infer.ipynb in this folder\n",
        "if not detections_csv.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Missing {detections_csv.resolve()}. Run `infer.ipynb` first to create the file.\"\n",
        "    )\n",
        "\n",
        "detections_df = pd.read_csv(detections_csv)\n",
        "detections_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "412b3ae5",
      "metadata": {},
      "outputs": [],
      "source": [
        "DEFAULT_CLASSES: Dict[int, str] = {\n",
        "    1: \"Topi\",\n",
        "    2: \"Buffalo\",\n",
        "    3: \"Kob\",\n",
        "    4: \"Warthog\",\n",
        "    5: \"Waterbuck\",\n",
        "    6: \"Elephant\",\n",
        "}\n",
        "\n",
        "def load_class_map(path: Optional[Path] = None) -> Dict[int, str]:\n",
        "    if path is None:\n",
        "        return DEFAULT_CLASSES\n",
        "    data = json.loads(path.read_text())\n",
        "    return {int(k): str(v) for k, v in data.items()}\n",
        "\n",
        "def extract_points(rows: pd.DataFrame) -> Tuple[List[Tuple[float, float]], List[int], List[float]]:\n",
        "    coords = [(float(row[\"x\"]), float(row[\"y\"])) for _, row in rows.iterrows()]\n",
        "    labels = [int(row[\"labels\"]) for _, row in rows.iterrows()]\n",
        "    scores = [float(row[\"scores\"]) for _, row in rows.iterrows()] if \"scores\" in rows.columns else []\n",
        "    return coords, labels, scores\n",
        "\n",
        "def evaluate_metrics(\n",
        "    gt_df: pd.DataFrame,\n",
        "    pred_df: pd.DataFrame,\n",
        "    class_map: Dict[int, str],\n",
        "    radius: float,\n",
        ") -> Dict[str, object]:\n",
        "    num_classes = len(class_map) + 1\n",
        "    metrics = PointsMetrics(radius=radius, num_classes=num_classes)\n",
        "\n",
        "    all_images = sorted(set(gt_df[\"images\"]) | set(pred_df[\"images\"]))\n",
        "\n",
        "    for image_name in all_images:\n",
        "        gt_rows = gt_df[gt_df[\"images\"] == image_name]\n",
        "        det_rows = pred_df[pred_df[\"images\"] == image_name]\n",
        "\n",
        "        gt_coords, gt_labels, _ = extract_points(gt_rows)\n",
        "        pred_coords, pred_labels, pred_scores = extract_points(det_rows)\n",
        "\n",
        "        est_count = [pred_labels.count(cls_id) for cls_id in range(1, num_classes)]\n",
        "\n",
        "        metrics.feed(\n",
        "            gt={\"loc\": gt_coords, \"labels\": gt_labels},\n",
        "            preds={\"loc\": pred_coords, \"labels\": pred_labels, \"scores\": pred_scores},\n",
        "            est_count=est_count,\n",
        "        )\n",
        "\n",
        "    per_class_metrics = metrics.copy()\n",
        "    metrics.aggregate()\n",
        "\n",
        "    overall = {\n",
        "        \"precision\": metrics.precision(),\n",
        "        \"recall\": metrics.recall(),\n",
        "        \"f1_score\": metrics.fbeta_score(),\n",
        "        \"mae\": metrics.mae(),\n",
        "        \"rmse\": metrics.rmse(),\n",
        "        \"mse\": metrics.mse(),\n",
        "        \"accuracy\": metrics.accuracy(),\n",
        "    }\n",
        "\n",
        "    per_class = {}\n",
        "    for class_id, class_name in class_map.items():\n",
        "        per_class[class_name] = {\n",
        "            \"precision\": per_class_metrics.precision(class_id),\n",
        "            \"recall\": per_class_metrics.recall(class_id),\n",
        "            \"f1_score\": per_class_metrics.fbeta_score(class_id),\n",
        "            \"mae\": per_class_metrics.mae(class_id),\n",
        "            \"rmse\": per_class_metrics.rmse(class_id),\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"overall\": overall,\n",
        "        \"per_class\": per_class,\n",
        "        \"classes\": class_map,\n",
        "        \"radius\": radius,\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3577f26c",
      "metadata": {},
      "outputs": [],
      "source": [
        "gt_csv = Path(\"../../../data-delplanque/test.csv\")  # Update with the ground truth CSV path\n",
        "radius = 5.0\n",
        "output_json = Path(\"metrics.json\")\n",
        "\n",
        "if not gt_csv.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Ground truth CSV not found at {gt_csv.resolve()}. Update the path before continuing.\"\n",
        "    )\n",
        "gt_df = pd.read_csv(gt_csv)\n",
        "class_map = load_class_map()\n",
        "\n",
        "summary = evaluate_metrics(\n",
        "    gt_df=gt_df,\n",
        "    pred_df=detections_df,\n",
        "    class_map=class_map,\n",
        "    radius=radius,\n",
        ")\n",
        "\n",
        "print(\"=== Metrics from CSVs ===\")\n",
        "print(json.dumps(summary[\"overall\"], indent=2))\n",
        "print(\"Per-class F1:\")\n",
        "for name, scores in summary[\"per_class\"].items():\n",
        "    print(\n",
        "        f\"  {name:10s} -> F1: {scores['f1_score']:.3f}, Recall: {scores['recall']:.3f}, Precision: {scores['precision']:.3f}\"\n",
        "    )\n",
        "\n",
        "if output_json is not None:\n",
        "    output_json.parent.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Metrics saved to {output_json}\")\n",
        "\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dc17b70",
      "metadata": {},
      "outputs": [],
      "source": [
        "detections_csv = Path(\"herdnet_baseline_phase_2_detections_overlap_160.csv\")  # Generated by infer.ipynb in this folder\n",
        "if not detections_csv.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Missing {detections_csv.resolve()}. Run `infer.ipynb` first to create the file.\"\n",
        "    )\n",
        "\n",
        "detections_df = pd.read_csv(detections_csv)\n",
        "detections_df.head()\n",
        "\n",
        "gt_csv = Path(\"../../../data-delplanque/test.csv\")  # Update with the ground truth CSV path\n",
        "radius = 5.0\n",
        "output_json = Path(\"metrics.json\")\n",
        "\n",
        "if not gt_csv.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Ground truth CSV not found at {gt_csv.resolve()}. Update the path before continuing.\"\n",
        "    )\n",
        "gt_df = pd.read_csv(gt_csv)\n",
        "class_map = load_class_map()\n",
        "\n",
        "summary = evaluate_metrics(\n",
        "    gt_df=gt_df,\n",
        "    pred_df=detections_df,\n",
        "    class_map=class_map,\n",
        "    radius=radius,\n",
        ")\n",
        "\n",
        "print(\"=== Metrics from CSVs ===\")\n",
        "print(json.dumps(summary[\"overall\"], indent=2))\n",
        "print(\"Per-class F1:\")\n",
        "for name, scores in summary[\"per_class\"].items():\n",
        "    print(\n",
        "        f\"  {name:10s} -> F1: {scores['f1_score']:.3f}, Recall: {scores['recall']:.3f}, Precision: {scores['precision']:.3f}\"\n",
        "    )\n",
        "\n",
        "if output_json is not None:\n",
        "    output_json.parent.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Metrics saved to {output_json}\")\n",
        "\n",
        "print(summary)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
