{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32259b98",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import json\n",
        "from animaloc.eval.metrics import PointsMetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a7702d6",
      "metadata": {},
      "source": [
        "# Evaluate From CSV\n",
        "\n",
        "This notebook computes HerdNet metrics from CSV files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d3348fc4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>images</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>labels</th>\n",
              "      <th>scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>S_07_05_16_DSC00162.JPG</td>\n",
              "      <td>811.892670</td>\n",
              "      <td>608.250519</td>\n",
              "      <td>2</td>\n",
              "      <td>0.942628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>S_07_05_16_DSC00163.JPG</td>\n",
              "      <td>1082.292419</td>\n",
              "      <td>2744.844727</td>\n",
              "      <td>2</td>\n",
              "      <td>0.968781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>S_07_05_16_DSC00163.JPG</td>\n",
              "      <td>1304.563293</td>\n",
              "      <td>2797.515869</td>\n",
              "      <td>2</td>\n",
              "      <td>0.394110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>S_07_05_16_DSC00307.JPG</td>\n",
              "      <td>3208.475586</td>\n",
              "      <td>16.589691</td>\n",
              "      <td>1</td>\n",
              "      <td>0.818842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>S_07_05_16_DSC00307.JPG</td>\n",
              "      <td>3463.140747</td>\n",
              "      <td>11.016659</td>\n",
              "      <td>1</td>\n",
              "      <td>0.293056</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    images            x            y  labels    scores\n",
              "0  S_07_05_16_DSC00162.JPG   811.892670   608.250519       2  0.942628\n",
              "1  S_07_05_16_DSC00163.JPG  1082.292419  2744.844727       2  0.968781\n",
              "2  S_07_05_16_DSC00163.JPG  1304.563293  2797.515869       2  0.394110\n",
              "3  S_07_05_16_DSC00307.JPG  3208.475586    16.589691       1  0.818842\n",
              "4  S_07_05_16_DSC00307.JPG  3463.140747    11.016659       1  0.293056"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "detections_csv = Path(\"detections.csv\")  # Generated by infer.ipynb in this folder\n",
        "if not detections_csv.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Missing {detections_csv.resolve()}. Run `infer.ipynb` first to create the file.\"\n",
        "    )\n",
        "\n",
        "detections_df = pd.read_csv(detections_csv)\n",
        "detections_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "412b3ae5",
      "metadata": {},
      "outputs": [],
      "source": [
        "DEFAULT_CLASSES: Dict[int, str] = {\n",
        "    1: \"Topi\",\n",
        "    2: \"Buffalo\",\n",
        "    3: \"Kob\",\n",
        "    4: \"Warthog\",\n",
        "    5: \"Waterbuck\",\n",
        "    6: \"Elephant\",\n",
        "}\n",
        "\n",
        "def load_class_map(path: Optional[Path] = None) -> Dict[int, str]:\n",
        "    if path is None:\n",
        "        return DEFAULT_CLASSES\n",
        "    data = json.loads(path.read_text())\n",
        "    return {int(k): str(v) for k, v in data.items()}\n",
        "\n",
        "def extract_points(rows: pd.DataFrame) -> Tuple[List[Tuple[float, float]], List[int], List[float]]:\n",
        "    coords = [(float(row[\"x\"]), float(row[\"y\"])) for _, row in rows.iterrows()]\n",
        "    labels = [int(row[\"labels\"]) for _, row in rows.iterrows()]\n",
        "    scores = [float(row[\"scores\"]) for _, row in rows.iterrows()] if \"scores\" in rows.columns else []\n",
        "    return coords, labels, scores\n",
        "\n",
        "def evaluate_metrics(\n",
        "    gt_df: pd.DataFrame,\n",
        "    pred_df: pd.DataFrame,\n",
        "    class_map: Dict[int, str],\n",
        "    radius: float,\n",
        ") -> Dict[str, object]:\n",
        "    num_classes = len(class_map) + 1\n",
        "    metrics = PointsMetrics(radius=radius, num_classes=num_classes)\n",
        "\n",
        "    all_images = sorted(set(gt_df[\"images\"]) | set(pred_df[\"images\"]))\n",
        "\n",
        "    for image_name in all_images:\n",
        "        gt_rows = gt_df[gt_df[\"images\"] == image_name]\n",
        "        det_rows = pred_df[pred_df[\"images\"] == image_name]\n",
        "\n",
        "        gt_coords, gt_labels, _ = extract_points(gt_rows)\n",
        "        pred_coords, pred_labels, pred_scores = extract_points(det_rows)\n",
        "\n",
        "        est_count = [pred_labels.count(cls_id) for cls_id in range(1, num_classes)]\n",
        "\n",
        "        metrics.feed(\n",
        "            gt={\"loc\": gt_coords, \"labels\": gt_labels},\n",
        "            preds={\"loc\": pred_coords, \"labels\": pred_labels, \"scores\": pred_scores},\n",
        "            est_count=est_count,\n",
        "        )\n",
        "\n",
        "    per_class_metrics = metrics.copy()\n",
        "    metrics.aggregate()\n",
        "\n",
        "    overall = {\n",
        "        \"precision\": metrics.precision(),\n",
        "        \"recall\": metrics.recall(),\n",
        "        \"f1_score\": metrics.fbeta_score(),\n",
        "        \"mae\": metrics.mae(),\n",
        "        \"rmse\": metrics.rmse(),\n",
        "        \"mse\": metrics.mse(),\n",
        "        \"accuracy\": metrics.accuracy(),\n",
        "    }\n",
        "\n",
        "    per_class = {}\n",
        "    for class_id, class_name in class_map.items():\n",
        "        per_class[class_name] = {\n",
        "            \"precision\": per_class_metrics.precision(class_id),\n",
        "            \"recall\": per_class_metrics.recall(class_id),\n",
        "            \"f1_score\": per_class_metrics.fbeta_score(class_id),\n",
        "            \"mae\": per_class_metrics.mae(class_id),\n",
        "            \"rmse\": per_class_metrics.rmse(class_id),\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"overall\": overall,\n",
        "        \"per_class\": per_class,\n",
        "        \"classes\": class_map,\n",
        "        \"radius\": radius,\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3577f26c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Metrics from CSVs ===\n",
            "{\n",
            "  \"precision\": 0.9384687646782527,\n",
            "  \"recall\": 0.8690735102218355,\n",
            "  \"f1_score\": 0.9024390243902439,\n",
            "  \"mae\": 1.1472868217054264,\n",
            "  \"rmse\": 2.4112141108520606,\n",
            "  \"mse\": 5.813953488372093,\n",
            "  \"accuracy\": 0.9584584584584585\n",
            "}\n",
            "Per-class F1:\n",
            "  Topi       -> F1: 0.896, Recall: 0.913, Precision: 0.880\n",
            "  Buffalo    -> F1: 0.843, Recall: 0.756, Precision: 0.953\n",
            "  Kob        -> F1: 0.905, Recall: 0.870, Precision: 0.943\n",
            "  Warthog    -> F1: 0.407, Recall: 0.338, Precision: 0.510\n",
            "  Waterbuck  -> F1: 0.710, Recall: 0.611, Precision: 0.846\n",
            "  Elephant   -> F1: 0.865, Recall: 0.833, Precision: 0.900\n",
            "Metrics saved to metrics.json\n",
            "{'overall': {'precision': 0.9384687646782527, 'recall': 0.8690735102218355, 'f1_score': 0.9024390243902439, 'mae': 1.1472868217054264, 'rmse': 2.4112141108520606, 'mse': 5.813953488372093, 'accuracy': np.float64(0.9584584584584585)}, 'per_class': {'Topi': {'precision': 0.88, 'recall': 0.9125925925925926, 'f1_score': 0.8959999999999999, 'mae': 1.5735294117647058, 'rmse': 2.917996895535179}, 'Buffalo': {'precision': 0.9530685920577617, 'recall': 0.7564469914040115, 'f1_score': 0.8434504792332268, 'mae': 3.3333333333333335, 'rmse': 6.493586579592718}, 'Kob': {'precision': 0.9431818181818182, 'recall': 0.870020964360587, 'f1_score': 0.9051254089422028, 'mae': 0.6914893617021277, 'rmse': 1.8071960178155857}, 'Warthog': {'precision': 0.5102040816326531, 'recall': 0.33783783783783783, 'f1_score': 0.4065040650406504, 'mae': 2.4814814814814814, 'rmse': 3.5013225014641622}, 'Waterbuck': {'precision': 0.8461538461538461, 'recall': 0.6111111111111112, 'f1_score': 0.7096774193548387, 'mae': 2.5714285714285716, 'rmse': 3.0237157840738176}, 'Elephant': {'precision': 0.8995290423861853, 'recall': 0.8328488372093024, 'f1_score': 0.8649056603773585, 'mae': 1.07, 'rmse': 1.5968719422671311}}, 'classes': {1: 'Topi', 2: 'Buffalo', 3: 'Kob', 4: 'Warthog', 5: 'Waterbuck', 6: 'Elephant'}, 'radius': 20.0}\n"
          ]
        }
      ],
      "source": [
        "gt_csv = Path(\"../../../data-delplanque/test.csv\")  # Update with the ground truth CSV path\n",
        "radius = 20.0\n",
        "output_json = Path(\"metrics.json\")\n",
        "\n",
        "if not gt_csv.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Ground truth CSV not found at {gt_csv.resolve()}. Update the path before continuing.\"\n",
        "    )\n",
        "gt_df = pd.read_csv(gt_csv)\n",
        "class_map = load_class_map()\n",
        "\n",
        "summary = evaluate_metrics(\n",
        "    gt_df=gt_df,\n",
        "    pred_df=detections_df,\n",
        "    class_map=class_map,\n",
        "    radius=radius,\n",
        ")\n",
        "\n",
        "print(\"=== Metrics from CSVs ===\")\n",
        "print(json.dumps(summary[\"overall\"], indent=2))\n",
        "print(\"Per-class F1:\")\n",
        "for name, scores in summary[\"per_class\"].items():\n",
        "    print(\n",
        "        f\"  {name:10s} -> F1: {scores['f1_score']:.3f}, Recall: {scores['recall']:.3f}, Precision: {scores['precision']:.3f}\"\n",
        "    )\n",
        "\n",
        "if output_json is not None:\n",
        "    output_json.parent.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Metrics saved to {output_json}\")\n",
        "\n",
        "print(summary)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
