{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d79638",
   "metadata": {},
   "source": [
    "# Fase 0 — Generación de Parches\n",
    "\n",
    "Dividimos las imágenes 24MP en parches cuadráticos y ajustamos las anotaciones COCO para obtener el dataset de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f64f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.8 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rfdetr import RFDETRNano\n",
    "\n",
    "from utils.common.bbox import convert_bbox_csv_to_points\n",
    "from utils.herdnet import evaluate_points_from_csv\n",
    "from utils.rf_detr import (\n",
    "    DEFAULT_CATEGORIES,\n",
    "    Detection,\n",
    "    DetectionSample,\n",
    "    HerdNetMetricsCallback,\n",
    "    PatchSummary,\n",
    "    SimpleStitcher,\n",
    "    generate_patch_dataset,\n",
    "    write_coco_predictions,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5c7438",
   "metadata": {},
   "source": [
    "## Configuración de splits\n",
    "\n",
    "Ajusta las rutas a tus directorios de origen (imágenes completas + JSON COCO) y el destino donde quedarán los parches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7baf3d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 384\n",
    "PATCH_OVERLAP = 160\n",
    "IMG_NORMALIZE_MEAN = [0.485, 0.456, 0.406]\n",
    "IMG_NORMALIZE_STD = [0.229, 0.224, 0.225]\n",
    "TRAIN_EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "CONF_THRESHOLD_STAGE1 = 0.5\n",
    "CONF_THRESHOLD_STAGE2 = 0.5\n",
    "MATCH_RADIUS = 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e96166",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_summaries = []\n",
    "for job in patch_jobs:\n",
    "    summary = generate_patch_dataset(\n",
    "        images_dir=job['images_dir'],\n",
    "        json_file=job['json_file'],\n",
    "        output_dir=job['output_dir'],\n",
    "        patch_width=job['patch_width'],\n",
    "        patch_height=job['patch_height'],\n",
    "        overlap=job['overlap'],\n",
    "        min_visibility=job['min_visibility'],\n",
    "    )\n",
    "    entry = {'split': job['split']}\n",
    "    entry.update(asdict(summary))\n",
    "    patch_summaries.append(entry)\n",
    "\n",
    "pd.DataFrame(patch_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb1ec5",
   "metadata": {},
   "source": [
    "# Fase 1 — Entrenamiento Inicial RF-DETR\n",
    "\n",
    "Entrenamos RF-DETR Nano sobre los parches generados y registramos métricas estilo HerdNet durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b9285",
   "metadata": {},
   "source": [
    "## Inicializar modelo y callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RFDETRNano()\n",
    "\n",
    "herdnet_callback = HerdNetMetricsCallback(\n",
    "    model=model,\n",
    "    val_dataset_path='data-nano-detr/valid/_annotations.coco.json',\n",
    "    val_images_dir='data-nano-detr/valid',\n",
    "    threshold_px=20,\n",
    "    confidence_threshold=0.5,\n",
    "    wandb_log=True,\n",
    "    eval_every_n_epochs=5,\n",
    ")\n",
    "\n",
    "model.callbacks['on_fit_epoch_end'].append(herdnet_callback.update)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd1c87c",
   "metadata": {},
   "source": [
    "## Entrenar (Stage 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b07c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\n",
    "    dataset_dir='data-nano-detr',\n",
    "    dataset_file='roboflow',\n",
    "    img_size=PATCH_SIZE,\n",
    "    epochs=TRAIN_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    grad_accum_steps=GRAD_ACCUM_STEPS,\n",
    "    output_dir='outputs/rfdetr_nano_stage1',\n",
    "    wandb=True,\n",
    "    project='rf-detr-nano',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a86d1",
   "metadata": {},
   "source": [
    "# Fase 1 — Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b9c322",
   "metadata": {},
   "source": [
    "## Configuración de inferencia y métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a95544e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cfg = OmegaConf.create({\n",
    "    'data': {\n",
    "        'images_root': 'data-delplanque/test',\n",
    "        'gt_points_csv': 'data-delplanque/test.csv',\n",
    "    },\n",
    "    'inference': {\n",
    "        'device': 'cuda',\n",
    "        'checkpoint_path': './outputs/rfdetr_nano_2/checkpoint_best_total.pth',\n",
    "        'threshold': CONF_THRESHOLD_STAGE1,\n",
    "        'batch_size': 16,\n",
    "        'output_path': './results/rfdetr_nano',\n",
    "        'detections_csv': 'rfdetr_stage1_detections.csv',\n",
    "    },\n",
    "    'metrics': {\n",
    "        'radius': MATCH_RADIUS,\n",
    "        'class_map': None,\n",
    "    },\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d9621",
   "metadata": {},
   "source": [
    "## Cargar checkpoint y preparar stitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357aef3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a different number of positional encodings than DINOv2, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.\n",
      "Using patch size 16 instead of 14, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.\n",
      "Loading pretrain weights\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(eval_cfg.inference.checkpoint_path, weights_only=False)\n",
    "state_dict = checkpoint.get('model', checkpoint.get('ema_model'))\n",
    "num_classes = state_dict['class_embed.weight'].shape[0]\n",
    "\n",
    "model_eval = RFDETRNano()\n",
    "model_eval.model.reinitialize_detection_head(num_classes)\n",
    "model_eval.model.model.load_state_dict(state_dict, strict=True)\n",
    "model_eval.model.model.to(eval_cfg.inference.device).eval()\n",
    "\n",
    "stitcher = SimpleStitcher(\n",
    "    model=model_eval.model.model,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    overlap=0,\n",
    "    batch_size=eval_cfg.inference.batch_size,\n",
    "    confidence_threshold=eval_cfg.inference.threshold,\n",
    "    device=eval_cfg.inference.device,\n",
    "    label_offset=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2400e39",
   "metadata": {},
   "source": [
    "## Ejecutar inferencia sobre imágenes completas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0e88be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 258/258 [03:19<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4292 detections to results/rfdetr_nano/rfdetr_stage1_detections.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "images_root = Path(eval_cfg.data.images_root)\n",
    "image_files = sorted(\n",
    "    list(images_root.glob('*.jpg'))\n",
    "    + list(images_root.glob('*.JPG'))\n",
    "    + list(images_root.glob('*.png'))\n",
    "    + list[Path](images_root.glob('*.PNG'))\n",
    ")\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "output_dir = Path(eval_cfg.inference.output_path)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "all_detections = []\n",
    "\n",
    "for img_path in tqdm(image_files, desc='Inference'):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image_tensor = transform(image=np.array(image))['image']\n",
    "    detections = stitcher(image_tensor)\n",
    "\n",
    "    for i in range(len(detections['scores'])):\n",
    "        all_detections.append({\n",
    "            'images': img_path.name,\n",
    "            'x': float(detections['boxes'][i, 0]),\n",
    "            'y': float(detections['boxes'][i, 1]),\n",
    "            'x_max': float(detections['boxes'][i, 2]),\n",
    "            'y_max': float(detections['boxes'][i, 3]),\n",
    "            'labels': int(detections['labels'][i]),\n",
    "            'scores': float(detections['scores'][i]),\n",
    "        })\n",
    "\n",
    "pd.DataFrame(all_detections).to_csv(output_dir / eval_cfg.inference.detections_csv, index=False)\n",
    "print('Saved', len(all_detections), 'detections to', output_dir / eval_cfg.inference.detections_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b112f",
   "metadata": {},
   "source": [
    "## Convertir detecciones a puntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c355d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4292 detections to points -> results/rfdetr_nano/rfdetr_stage1_detections_points.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>labels</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01802f75da35434ab373569fffc1fd65a3417aef.JPG</td>\n",
       "      <td>1472.237793</td>\n",
       "      <td>262.071701</td>\n",
       "      <td>6</td>\n",
       "      <td>0.513108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01802f75da35434ab373569fffc1fd65a3417aef.JPG</td>\n",
       "      <td>4673.474609</td>\n",
       "      <td>251.195457</td>\n",
       "      <td>6</td>\n",
       "      <td>0.667662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01802f75da35434ab373569fffc1fd65a3417aef.JPG</td>\n",
       "      <td>2985.883301</td>\n",
       "      <td>906.458740</td>\n",
       "      <td>6</td>\n",
       "      <td>0.581643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01802f75da35434ab373569fffc1fd65a3417aef.JPG</td>\n",
       "      <td>5190.343262</td>\n",
       "      <td>1105.431396</td>\n",
       "      <td>6</td>\n",
       "      <td>0.693571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01802f75da35434ab373569fffc1fd65a3417aef.JPG</td>\n",
       "      <td>1168.424866</td>\n",
       "      <td>1482.091553</td>\n",
       "      <td>6</td>\n",
       "      <td>0.745156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         images            x            y  \\\n",
       "0  01802f75da35434ab373569fffc1fd65a3417aef.JPG  1472.237793   262.071701   \n",
       "1  01802f75da35434ab373569fffc1fd65a3417aef.JPG  4673.474609   251.195457   \n",
       "2  01802f75da35434ab373569fffc1fd65a3417aef.JPG  2985.883301   906.458740   \n",
       "3  01802f75da35434ab373569fffc1fd65a3417aef.JPG  5190.343262  1105.431396   \n",
       "4  01802f75da35434ab373569fffc1fd65a3417aef.JPG  1168.424866  1482.091553   \n",
       "\n",
       "   labels    scores  \n",
       "0       6  0.513108  \n",
       "1       6  0.667662  \n",
       "2       6  0.581643  \n",
       "3       6  0.693571  \n",
       "4       6  0.745156  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_path = output_dir / 'rfdetr_stage1_detections_points.csv'\n",
    "points_df = convert_bbox_csv_to_points(\n",
    "    output_dir / eval_cfg.inference.detections_csv,\n",
    "    points_path,\n",
    ")\n",
    "print('Converted', len(points_df), 'detections to points ->', points_path)\n",
    "points_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a61263",
   "metadata": {},
   "source": [
    "## Calcular métricas HerdNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summary = evaluate_points_from_csv(\n",
    "    gt_csv=eval_cfg.data.gt_points_csv,\n",
    "    detections_csv=points_path,\n",
    "    class_map_path=eval_cfg.metrics.class_map,\n",
    "    radius=eval_cfg.metrics.radius,\n",
    ")\n",
    "metrics_summary['overall']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcbb6c9",
   "metadata": {},
   "source": [
    "# Fase 2 — Hard Negatives y Stage 2\n",
    "\n",
    "Usamos el modelo de la fase 1 para recolectar falsos positivos, generar parches negativos y reentrenar RF-DETR con el dataset ampliado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21908a4e",
   "metadata": {},
   "source": [
    "## Inferencia sobre el split de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20b9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "hn_cfg = OmegaConf.create({\n",
    "    'data': {\n",
    "        'train_root': 'data/herdnet/raw/train',\n",
    "    },\n",
    "    'inference': {\n",
    "        'device': 'cuda',\n",
    "        'checkpoint_path': './outputs/rfdetr_nano_stage1/checkpoint_phase_1.pth',\n",
    "        'threshold': 0.1,\n",
    "        'batch_size': 2,\n",
    "        'output_path': './results/hnp_stage1',\n",
    "        'detections_csv': 'rfdetr_stage1_train_detections.csv',\n",
    "        'detections_json': 'rfdetr_stage1_train_detections.json',\n",
    "    },\n",
    "    'patches': {\n",
    "        'patch_width': 384,\n",
    "        'patch_height': 384,\n",
    "        'overlap': 160,\n",
    "        'min_visibility': 0.5,\n",
    "        'output_dir': 'data-nano-detr/hnp',\n",
    "    },\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hn_checkpoint = torch.load(hn_cfg.inference.checkpoint_path, weights_only=False)\n",
    "hn_state = hn_checkpoint.get('model', hn_checkpoint.get('ema_model'))\n",
    "hn_num_classes = hn_state['class_embed.weight'].shape[0] - 1\n",
    "\n",
    "hn_model = RFDETRNano()\n",
    "hn_model.model.reinitialize_detection_head(hn_num_classes)\n",
    "hn_model.model.model.load_state_dict(hn_state, strict=True)\n",
    "hn_model.model.model.to(hn_cfg.inference.device).eval()\n",
    "\n",
    "hn_stitcher = SimpleStitcher(\n",
    "    model=hn_model.model.model,\n",
    "    patch_size=hn_cfg.patches.patch_width,\n",
    "    overlap=0,\n",
    "    batch_size=hn_cfg.inference.batch_size,\n",
    "    confidence_threshold=hn_cfg.inference.threshold,\n",
    "    device=hn_cfg.inference.device,\n",
    "    label_offset=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51540c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = Path(hn_cfg.data.train_root)\n",
    "train_images = sorted(\n",
    "    list(train_root.glob('*.jpg')) +\n",
    "    list(train_root.glob('*.JPG')) +\n",
    "    list(train_root.glob('*.png')) +\n",
    "    list(train_root.glob('*.PNG'))\n",
    ")\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Normalize(mean=IMG_NORMALIZE_MEAN, std=IMG_NORMALIZE_STD),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "hn_output_dir = Path(hn_cfg.inference.output_path)\n",
    "hn_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "hn_records = []\n",
    "hn_samples = []\n",
    "\n",
    "for img_path in tqdm(train_images, desc='HN inference'):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    width, height = image.size\n",
    "    tensor = transform(image=np.array(image))['image']\n",
    "    detections = hn_stitcher(tensor)\n",
    "\n",
    "    det_list = []\n",
    "    for i in range(len(detections['scores'])):\n",
    "        x1, y1, x2, y2 = detections['boxes'][i].tolist()\n",
    "        label = int(detections['labels'][i])\n",
    "        score = float(detections['scores'][i])\n",
    "        hn_records.append({\n",
    "            'images': img_path.name,\n",
    "            'x': x1,\n",
    "            'y': y1,\n",
    "            'x_max': x2,\n",
    "            'y_max': y2,\n",
    "            'labels': label,\n",
    "            'scores': score,\n",
    "        })\n",
    "        det_list.append(Detection(bbox=[x1, y1, x2, y2], label=label, score=score))\n",
    "\n",
    "    hn_samples.append(DetectionSample(file_name=img_path.name, width=width, height=height, detections=det_list))\n",
    "\n",
    "hn_df = pd.DataFrame(hn_records)\n",
    "hn_csv_path = hn_output_dir / hn_cfg.inference.detections_csv\n",
    "hn_df.to_csv(hn_csv_path, index=False)\n",
    "write_coco_predictions(hn_samples, hn_output_dir / hn_cfg.inference.detections_json)\n",
    "hn_csv_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739a9603",
   "metadata": {},
   "source": [
    "## Generar parches negativos\n",
    "\n",
    "Utilizamos el JSON de detecciones para recortar parches alrededor de los falsos positivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a53f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hnp_summary = generate_patch_dataset(\n",
    "    images_dir=hn_cfg.data.train_root,\n",
    "    json_file=hn_output_dir / hn_cfg.inference.detections_json,\n",
    "    output_dir=hn_cfg.patches.output_dir,\n",
    "    patch_width=hn_cfg.patches.patch_width,\n",
    "    patch_height=hn_cfg.patches.patch_height,\n",
    "    overlap=hn_cfg.patches.overlap,\n",
    "    min_visibility=hn_cfg.patches.min_visibility,\n",
    ")\n",
    "pd.DataFrame([asdict(hnp_summary)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce6dcd5",
   "metadata": {},
   "source": [
    "> Nota: para obtener verdaderos hard negatives conviene filtrar `hn_df` para eliminar detecciones que coinciden con el ground truth (true positives). Se puede cruzar con las anotaciones originales o con métricas de puntos antes de llamar a `generate_patch_dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddec4d64",
   "metadata": {},
   "source": [
    "## Combinar parches originales + HNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d89fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copy2\n",
    "\n",
    "stage2_root = Path('data-nano-detr-stage2/train')\n",
    "stage2_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "original_patches = Path('data-nano-detr/train')\n",
    "hnp_patches = Path(hn_cfg.patches.output_dir)\n",
    "\n",
    "for src in original_patches.glob('*.jpg'):\n",
    "    copy2(src, stage2_root / src.name)\n",
    "\n",
    "for src in hnp_patches.glob('*.jpg'):\n",
    "    dst = stage2_root / src.name\n",
    "    if not dst.exists():\n",
    "        copy2(src, dst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2c800c",
   "metadata": {},
   "source": [
    "Genera un nuevo `_annotations.coco.json` para Stage 2 fusionando el JSON original con los parches HNP. Puedes reutilizar `generate_patch_dataset` o construirlo manualmente con `write_coco_predictions` dependiendo de si mantienes etiquetas explícitas para los negativos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba224f70",
   "metadata": {},
   "source": [
    "## Entrenar (Stage 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stage2 = RFDETRNano()\n",
    "\n",
    "herdnet_callback_stage2 = HerdNetMetricsCallback(\n",
    "    model=model_stage2,\n",
    "    val_dataset_path='data-nano-detr/valid/_annotations.coco.json',\n",
    "    val_images_dir='data-nano-detr/valid',\n",
    "    threshold_px=20,\n",
    "    confidence_threshold=0.5,\n",
    "    wandb_log=True,\n",
    "    eval_every_n_epochs=5,\n",
    ")\n",
    "\n",
    "model_stage2.callbacks['on_fit_epoch_end'].append(herdnet_callback_stage2.update)\n",
    "\n",
    "model_stage2.train(\n",
    "    dataset_dir='data-nano-detr-stage2',\n",
    "    dataset_file='roboflow',\n",
    "    img_size=PATCH_SIZE,\n",
    "    epochs=TRAIN_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    grad_accum_steps=GRAD_ACCUM_STEPS,\n",
    "    output_dir='outputs/rfdetr_nano_stage2',\n",
    "    wandb=True,\n",
    "    project='rf-detr-nano-stage2',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4c371",
   "metadata": {},
   "source": [
    "Antes de calcular métricas, repite la celda de inferencia completa cambiando `eval_cfg.inference.checkpoint_path`, `output_path` y nombres de archivos para que apunten al checkpoint de Stage 2 (`outputs/rfdetr_nano_stage2`). Luego vuelve a ejecutar la conversión a puntos (`convert_bbox_csv_to_points`) para generar `./results/stage2/rfdetr_stage2_detections_points.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce6f1ef",
   "metadata": {},
   "source": [
    "## Evaluar Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_metrics = evaluate_points_from_csv(\n",
    "    gt_csv=eval_cfg.data.gt_points_csv,\n",
    "    detections_csv=Path('./results/stage2') / 'rfdetr_stage2_detections_points.csv',\n",
    "    class_map_path=eval_cfg.metrics.class_map,\n",
    "    radius=eval_cfg.metrics.radius,\n",
    ")\n",
    "stage2_metrics['overall']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
