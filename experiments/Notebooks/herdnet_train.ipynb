{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HerdNet Training Pipeline\n",
        "\n",
        "Pipeline completo de entrenamiento de HerdNet siguiendo la metodolog√≠a de Delplanque et al. (2023):\n",
        "\n",
        "1. **Fase 0**: Generaci√≥n de parches de entrenamiento y validaci√≥n\n",
        "2. **Fase 1**: Entrenamiento inicial (Stage 1) sobre parches\n",
        "3. **Fase 2**: Generaci√≥n de Hard Negative Patches (HNPs)\n",
        "4. **Fase 3**: Entrenamiento con HNPs (Stage 2)\n",
        "5. **Fase 4**: Evaluaci√≥n final sobre im√°genes completas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from shutil import copy2\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "from dataclasses import asdict\n",
        "\n",
        "from utils.herdnet import (\n",
        "    TrainConfig,\n",
        "    train_stage1,\n",
        "    train_stage2,\n",
        "    HNPConfig,\n",
        "    generate_hard_negative_patches,\n",
        "    EvalConfig,\n",
        "    evaluate_full_images,\n",
        "    evaluate_points_from_csv,\n",
        ")\n",
        "\n",
        "from utils.rf_detr import generate_patch_dataset, PatchSummary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuraci√≥n Global\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuraci√≥n de paths\n",
        "DATA_ROOT = Path(\"data-delplanque\")\n",
        "OUTPUT_ROOT = Path(\"outputs/herdnet\")\n",
        "\n",
        "# Configuraci√≥n de patches\n",
        "PATCH_SIZE = 512\n",
        "PATCH_OVERLAP = 160\n",
        "MIN_VISIBILITY = 0.1\n",
        "\n",
        "# Configuraci√≥n de entrenamiento\n",
        "BATCH_SIZE = 4\n",
        "NUM_WORKERS = 4\n",
        "EPOCHS_STAGE1 = 100\n",
        "EPOCHS_STAGE2 = 50\n",
        "LR_STAGE1 = 1e-4\n",
        "LR_STAGE2 = 1e-6\n",
        "\n",
        "# Configuraci√≥n de evaluaci√≥n\n",
        "MATCH_RADIUS = 5.0\n",
        "STITCH_OVERLAP = 160\n",
        "\n",
        "# WandB (opcional)\n",
        "WANDB_PROJECT = None  # \"herdnet-training\"\n",
        "WANDB_ENTITY = None\n",
        "WANDB_MODE = \"disabled\"  # \"online\" para activar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fase 0 ‚Äî Generaci√≥n de Parches\n",
        "\n",
        "Dividimos las im√°genes de alta resoluci√≥n (24MP) en parches de 512√ó512 p√≠xeles para facilitar el entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "patch_jobs = [\n",
        "    {\n",
        "        \"split\": \"train\",\n",
        "        \"images_dir\": DATA_ROOT / \"train\",\n",
        "        \"json_file\": DATA_ROOT / \"train.json\",\n",
        "        \"output_dir\": OUTPUT_ROOT / \"patches\" / \"train\",\n",
        "        \"patch_width\": PATCH_SIZE,\n",
        "        \"patch_height\": PATCH_SIZE,\n",
        "        \"overlap\": PATCH_OVERLAP,\n",
        "        \"min_visibility\": MIN_VISIBILITY,\n",
        "    },\n",
        "    {\n",
        "        \"split\": \"val\",\n",
        "        \"images_dir\": DATA_ROOT / \"val\",\n",
        "        \"json_file\": DATA_ROOT / \"val.json\",\n",
        "        \"output_dir\": OUTPUT_ROOT / \"patches\" / \"val\",\n",
        "        \"patch_width\": PATCH_SIZE,\n",
        "        \"patch_height\": PATCH_SIZE,\n",
        "        \"overlap\": PATCH_OVERLAP,\n",
        "        \"min_visibility\": MIN_VISIBILITY,\n",
        "    },\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "patch_summaries = []\n",
        "\n",
        "for job in patch_jobs:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Generando parches: {job['split']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    summary = generate_patch_dataset(\n",
        "        images_dir=job[\"images_dir\"],\n",
        "        json_file=job[\"json_file\"],\n",
        "        output_dir=job[\"output_dir\"],\n",
        "        patch_width=job[\"patch_width\"],\n",
        "        patch_height=job[\"patch_height\"],\n",
        "        overlap=job[\"overlap\"],\n",
        "        min_visibility=job[\"min_visibility\"],\n",
        "        include_background_category=True,\n",
        "    )\n",
        "    \n",
        "    entry = {\"split\": job[\"split\"]}\n",
        "    entry.update(asdict(summary))\n",
        "    patch_summaries.append(entry)\n",
        "    \n",
        "    print(f\"‚úì Parches creados: {summary.patches_created}\")\n",
        "    print(f\"‚úì Anotaciones: {summary.annotations_patches}\")\n",
        "\n",
        "pd.DataFrame(patch_summaries)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convertir anotaciones COCO a CSV\n",
        "\n",
        "HerdNet requiere formato CSV con columnas: `images`, `x`, `y`, `labels`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def coco_to_csv(coco_json: Path, output_csv: Path) -> pd.DataFrame:\n",
        "    \"\"\"Convierte anotaciones COCO a formato CSV para HerdNet.\"\"\"\n",
        "    with coco_json.open(\"r\") as f:\n",
        "        coco = json.load(f)\n",
        "    \n",
        "    # Mapear image_id a file_name\n",
        "    image_map = {img[\"id\"]: img[\"file_name\"] for img in coco[\"images\"]}\n",
        "    \n",
        "    # Extraer anotaciones\n",
        "    records = []\n",
        "    for ann in coco[\"annotations\"]:\n",
        "        x, y, w, h = ann[\"bbox\"]\n",
        "        cx, cy = x + w / 2, y + h / 2\n",
        "        records.append({\n",
        "            \"images\": image_map[ann[\"image_id\"]],\n",
        "            \"x\": cx,\n",
        "            \"y\": cy,\n",
        "            \"labels\": ann[\"category_id\"],\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(records)\n",
        "    output_csv.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"‚úì Guardado CSV: {output_csv} ({len(df)} anotaciones)\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convertir train patches\n",
        "train_csv = coco_to_csv(\n",
        "    OUTPUT_ROOT / \"patches\" / \"train\" / \"_annotations.coco.json\",\n",
        "    OUTPUT_ROOT / \"patches\" / \"train\" / \"gt.csv\",\n",
        ")\n",
        "\n",
        "# Convertir val patches\n",
        "val_csv = coco_to_csv(\n",
        "    OUTPUT_ROOT / \"patches\" / \"val\" / \"_annotations.coco.json\",\n",
        "    OUTPUT_ROOT / \"patches\" / \"val\" / \"gt.csv\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fase 1 ‚Äî Entrenamiento Stage 1\n",
        "\n",
        "Entrenar HerdNet sobre los parches generados. Este es el entrenamiento inicial sin Hard Negative Patches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stage1_config = TrainConfig(\n",
        "    train_root=OUTPUT_ROOT / \"patches\" / \"train\",\n",
        "    train_csv=OUTPUT_ROOT / \"patches\" / \"train\" / \"gt.csv\",\n",
        "    val_root=OUTPUT_ROOT / \"patches\" / \"val\",\n",
        "    val_csv=OUTPUT_ROOT / \"patches\" / \"val\" / \"gt.csv\",\n",
        "    work_dir=OUTPUT_ROOT / \"stage1\",\n",
        "    epochs=EPOCHS_STAGE1,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=LR_STAGE1,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    stitch_overlap=STITCH_OVERLAP,\n",
        "    wandb_project=WANDB_PROJECT,\n",
        "    wandb_entity=WANDB_ENTITY,\n",
        "    wandb_mode=WANDB_MODE,\n",
        "    wandb_run_name=\"herdnet_stage1\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INICIANDO ENTRENAMIENTO STAGE 1\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "stage1_result = train_stage1(stage1_config)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STAGE 1 COMPLETADO\")\n",
        "print(\"=\"*70)\n",
        "print(f\"‚úì Best checkpoint: {stage1_result.best_checkpoint}\")\n",
        "print(f\"‚úì Latest checkpoint: {stage1_result.latest_checkpoint}\")\n",
        "print(\"=\"*70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fase 2 ‚Äî Generaci√≥n de Hard Negative Patches\n",
        "\n",
        "Usar el modelo de Stage 1 para generar predicciones sobre las im√°genes de entrenamiento completas y extraer parches de falsos positivos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparar CSV de im√°genes completas\n",
        "\n",
        "Necesitamos un CSV con las anotaciones de las im√°genes completas de entrenamiento (no los parches).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convertir anotaciones de train (im√°genes completas) a CSV\n",
        "train_full_csv = coco_to_csv(\n",
        "    DATA_ROOT / \"train.json\",\n",
        "    OUTPUT_ROOT / \"train_full.csv\",\n",
        ")\n",
        "\n",
        "train_full_csv.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generar HNPs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hnp_config = HNPConfig(\n",
        "    checkpoint=stage1_result.best_checkpoint,\n",
        "    train_csv=OUTPUT_ROOT / \"train_full.csv\",\n",
        "    train_root=DATA_ROOT / \"train\",\n",
        "    output_root=OUTPUT_ROOT / \"hnp_patches\",\n",
        "    patch_size=PATCH_SIZE,\n",
        "    patch_overlap=PATCH_OVERLAP,\n",
        "    min_score=0.0,  # Incluir todas las detecciones\n",
        "    batch_size=1,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERANDO HARD NEGATIVE PATCHES\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "hnp_result = generate_hard_negative_patches(hnp_config)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"HNP GENERATION COMPLETADO\")\n",
        "print(\"=\"*70)\n",
        "print(f\"‚úì Parches HNP creados: {hnp_result.hnp_patches_created}\")\n",
        "print(f\"‚úì Detecciones CSV: {hnp_result.detections_csv}\")\n",
        "print(f\"‚úì Output dir: {hnp_result.output_root}\")\n",
        "print(\"=\"*70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combinar parches originales con HNPs para Stage 2\n",
        "\n",
        "Copiar todos los parches originales de Stage 1 y a√±adir los HNPs generados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stage2_train_dir = OUTPUT_ROOT / \"patches_stage2\" / \"train\"\n",
        "stage2_train_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Copiar parches originales de Stage 1\n",
        "print(\"Copiando parches originales de Stage 1...\")\n",
        "stage1_train_dir = OUTPUT_ROOT / \"patches\" / \"train\"\n",
        "copied_original = 0\n",
        "\n",
        "for pattern in (\"*.jpg\", \"*.JPG\", \"*.png\", \"*.PNG\"):\n",
        "    for src in stage1_train_dir.glob(pattern):\n",
        "        dst = stage2_train_dir / src.name\n",
        "        if not dst.exists():\n",
        "            copy2(src, dst)\n",
        "            copied_original += 1\n",
        "\n",
        "print(f\"‚úì Copiados {copied_original} parches originales\")\n",
        "\n",
        "# Copiar HNPs\n",
        "print(\"\\nCopiando HNP patches...\")\n",
        "hnp_dir = OUTPUT_ROOT / \"hnp_patches\"\n",
        "copied_hnp = 0\n",
        "\n",
        "for pattern in (\"*.jpg\", \"*.JPG\", \"*.png\", \"*.PNG\"):\n",
        "    for src in hnp_dir.glob(pattern):\n",
        "        dst = stage2_train_dir / src.name\n",
        "        if not dst.exists():\n",
        "            copy2(src, dst)\n",
        "            copied_hnp += 1\n",
        "\n",
        "print(f\"‚úì Copiados {copied_hnp} HNP patches\")\n",
        "print(f\"\\n‚úì Total Stage 2 patches: {copied_original + copied_hnp}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**IMPORTANTE**: Para Stage 2, usamos el CSV original de Stage 1 (gt.csv), NO el gt.csv generado por HNP.\n",
        "\n",
        "Los patches que no est√°n en el CSV ser√°n tratados autom√°ticamente como background por `FolderDataset`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Usar el GT original de stage1 (NO el de HNP)\n",
        "copy2(\n",
        "    OUTPUT_ROOT / \"patches\" / \"train\" / \"gt.csv\",\n",
        "    stage2_train_dir / \"gt.csv\",\n",
        ")\n",
        "\n",
        "print(f\"‚úì CSV de Stage 2 listo: {stage2_train_dir / 'gt.csv'}\")\n",
        "print(\"  (Contiene solo anotaciones originales; HNPs son background)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fase 3 ‚Äî Entrenamiento Stage 2\n",
        "\n",
        "Entrenar con los parches originales + HNPs usando una tasa de aprendizaje m√°s baja.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stage2_config = TrainConfig(\n",
        "    train_root=stage2_train_dir,\n",
        "    train_csv=stage2_train_dir / \"gt.csv\",\n",
        "    val_root=OUTPUT_ROOT / \"patches\" / \"val\",\n",
        "    val_csv=OUTPUT_ROOT / \"patches\" / \"val\" / \"gt.csv\",\n",
        "    work_dir=OUTPUT_ROOT / \"stage2\",\n",
        "    epochs=EPOCHS_STAGE2,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=LR_STAGE2,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    stitch_overlap=STITCH_OVERLAP,\n",
        "    wandb_project=WANDB_PROJECT,\n",
        "    wandb_entity=WANDB_ENTITY,\n",
        "    wandb_mode=WANDB_MODE,\n",
        "    wandb_run_name=\"herdnet_stage2\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INICIANDO ENTRENAMIENTO STAGE 2\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "stage2_result = train_stage2(\n",
        "    config=stage2_config,\n",
        "    stage1_checkpoint=stage1_result.best_checkpoint,\n",
        "    learning_rate=LR_STAGE2,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STAGE 2 COMPLETADO\")\n",
        "print(\"=\"*70)\n",
        "print(f\"‚úì Best checkpoint: {stage2_result.best_checkpoint}\")\n",
        "print(f\"‚úì Latest checkpoint: {stage2_result.latest_checkpoint}\")\n",
        "print(\"=\"*70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fase 4 ‚Äî Evaluaci√≥n Final\n",
        "\n",
        "Evaluar el modelo Stage 2 sobre im√°genes completas de validaci√≥n/test.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparar CSV de test (im√°genes completas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convertir anotaciones de test (im√°genes completas) a CSV\n",
        "test_csv = coco_to_csv(\n",
        "    DATA_ROOT / \"test.json\",\n",
        "    OUTPUT_ROOT / \"test_full.csv\",\n",
        ")\n",
        "\n",
        "test_csv.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluar Stage 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_stage1_config = EvalConfig(\n",
        "    checkpoint=stage1_result.best_checkpoint,\n",
        "    csv=OUTPUT_ROOT / \"test_full.csv\",\n",
        "    root=DATA_ROOT / \"test\",\n",
        "    output_dir=OUTPUT_ROOT / \"eval_stage1\",\n",
        "    patch_size=PATCH_SIZE,\n",
        "    overlap=STITCH_OVERLAP,\n",
        "    upsample=True,\n",
        "    match_radius=MATCH_RADIUS,\n",
        "    batch_size=1,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUANDO STAGE 1\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "eval_stage1_result = evaluate_full_images(eval_stage1_config)\n",
        "\n",
        "print(\"\\nStage 1 Metrics:\")\n",
        "pd.DataFrame([eval_stage1_result.metrics[\"overall\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluar Stage 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_stage2_config = EvalConfig(\n",
        "    checkpoint=stage2_result.best_checkpoint,\n",
        "    csv=OUTPUT_ROOT / \"test_full.csv\",\n",
        "    root=DATA_ROOT / \"test\",\n",
        "    output_dir=OUTPUT_ROOT / \"eval_stage2\",\n",
        "    patch_size=PATCH_SIZE,\n",
        "    overlap=STITCH_OVERLAP,\n",
        "    upsample=True,\n",
        "    match_radius=MATCH_RADIUS,\n",
        "    batch_size=1,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUANDO STAGE 2\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "eval_stage2_result = evaluate_full_images(eval_stage2_config)\n",
        "\n",
        "print(\"\\nStage 2 Metrics:\")\n",
        "pd.DataFrame([eval_stage2_result.metrics[\"overall\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "comparison = pd.DataFrame([\n",
        "    {\"Stage\": \"Stage 1\", **eval_stage1_result.metrics[\"overall\"]},\n",
        "    {\"Stage\": \"Stage 2\", **eval_stage2_result.metrics[\"overall\"]},\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPARACI√ìN STAGE 1 vs STAGE 2\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## M√©tricas por clase (Stage 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "per_class_df = pd.DataFrame(eval_stage2_result.metrics[\"per_class\"]).T\n",
        "\n",
        "print(\"\\nM√©tricas por clase (Stage 2):\")\n",
        "per_class_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Resumen Final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PIPELINE COMPLETO FINALIZADO\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüìÅ Checkpoints:\")\n",
        "print(f\"  Stage 1: {stage1_result.best_checkpoint}\")\n",
        "print(f\"  Stage 2: {stage2_result.best_checkpoint}\")\n",
        "\n",
        "print(\"\\nüìä Detecciones:\")\n",
        "print(f\"  Stage 1: {eval_stage1_result.detections_csv}\")\n",
        "print(f\"  Stage 2: {eval_stage2_result.detections_csv}\")\n",
        "\n",
        "print(\"\\nüìà M√©tricas:\")\n",
        "print(f\"  Stage 1 F1: {eval_stage1_result.metrics['overall']['f1_score']:.4f}\")\n",
        "print(f\"  Stage 2 F1: {eval_stage2_result.metrics['overall']['f1_score']:.4f}\")\n",
        "print(f\"  Mejora: {(eval_stage2_result.metrics['overall']['f1_score'] - eval_stage1_result.metrics['overall']['f1_score']):.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
