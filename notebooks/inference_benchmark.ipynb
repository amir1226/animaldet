{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f787204d",
   "metadata": {},
   "source": [
    "# Benchmark de inferencia (RF-DETR)\n",
    "\n",
    "Este notebook calcula tiempos de inferencia por imagen sobre un conjunto de ejemplo y resume media, mediana y percentil 95.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74c4d0",
   "metadata": {},
   "source": [
    "## Flujo\n",
    "\n",
    "1. Configurar rutas del checkpoint y del directorio con imágenes.\n",
    "2. Cargar el modelo RF-DETR y crear un `SimpleStitcher` con ventana de 560 px.\n",
    "3. Medir tiempos sobre hasta 100 imágenes (con warmup opcional) sincronizando el dispositivo.\n",
    "4. Mostrar distribución (tabla + métricas resumen).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e397aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.8 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import statistics\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "REPO_ROOT = Path.cwd()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Parámetros a ajustar\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "CHECKPOINT_PATH = REPO_ROOT / \"outputs/rfdetr_small/checkpoint_512_small_phase2.pth\"\n",
    "IMAGES_DIR = REPO_ROOT / \"data-delplanque\" / \"test\"  # Directorio de imágenes\n",
    "MAX_IMAGES = 100\n",
    "WARMUP_IMAGES = 5\n",
    "PATCH_SIZE = 512\n",
    "PATCH_OVERLAP = 0\n",
    "BATCH_SIZE = 32\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MEAN = (0.485, 0.456, 0.406)\n",
    "STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "print(f\"Dispositivo: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca853ed",
   "metadata": {},
   "source": [
    "### Cargar modelo y stitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f164506d-9282-4d9c-ae5e-9c8cfe79a501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a different number of positional encodings than DINOv2, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.\n",
      "Using patch size 16 instead of 14, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.\n",
      "Loading pretrain weights\n"
     ]
    }
   ],
   "source": [
    "from rfdetr import RFDETRSmall\n",
    "\n",
    "from utils.rf_detr import SimpleStitcher\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "state_dict = checkpoint.get('model', checkpoint.get('ema_model'))\n",
    "num_classes = state_dict['class_embed.weight'].shape[0]\n",
    "\n",
    "model_eval = RFDETRSmall()\n",
    "model_eval.model.reinitialize_detection_head(num_classes)\n",
    "model_eval.model.model.load_state_dict(state_dict, strict=True)\n",
    "model_eval.model.model.to(DEVICE).eval()\n",
    "\n",
    "stitcher = SimpleStitcher(\n",
    "    model=model_eval.model.model,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    overlap=PATCH_OVERLAP,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    confidence_threshold=CONFIDENCE_THRESHOLD,\n",
    "    device=DEVICE,\n",
    "    label_offset=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ef987",
   "metadata": {},
   "source": [
    "### Selección de imágenes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f1790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imágenes totales encontradas: 258\n",
      "Usaremos 105 (incluye warmup)\n",
      "First sample: /mnt/batch/tasks/shared/LS_root/mounts/clusters/asg-lab/code/Users/amir.sadour/proyecto/data-delplanque/test/01802f75da35434ab373569fffc1fd65a3417aef.JPG\n"
     ]
    }
   ],
   "source": [
    "image_paths = sorted([p for p in IMAGES_DIR.iterdir() if p.suffix.lower() in {'.jpg', '.jpeg', '.png'}])\n",
    "if not image_paths:\n",
    "    raise RuntimeError(f\"No se encontraron imágenes en {IMAGES_DIR}\")\n",
    "\n",
    "selected = image_paths[: min(len(image_paths), MAX_IMAGES + WARMUP_IMAGES)]\n",
    "print(f\"Imágenes totales encontradas: {len(image_paths)}\")\n",
    "print(f\"Usaremos {len(selected)} (incluye warmup)\")\n",
    "\n",
    "# Pre-cargar a memoria para evitar medir IO en cada iteración\n",
    "loaded_images: List[tuple[Path, np.ndarray]] = []\n",
    "for path in selected:\n",
    "    arr = np.array(Image.open(path).convert('RGB'))\n",
    "    loaded_images.append((path, arr))\n",
    "\n",
    "print('First sample:', loaded_images[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77eee86",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "Sincronizamos el dispositivo tras cada inferencia para capturar el tiempo real de finalización.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e45776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencias medidas: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>seconds</th>\n",
       "      <th>milliseconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04bda8c273b5ef6b29e1f318a1da3e7506a5e4d8.JPG</td>\n",
       "      <td>0.170163</td>\n",
       "      <td>170.162891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04e8092d743bef891386f3e0ce82155f12aa4035.JPG</td>\n",
       "      <td>0.169499</td>\n",
       "      <td>169.498501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04f53ca293e5037fc04f61e49b372d8f003c482d.JPG</td>\n",
       "      <td>0.381622</td>\n",
       "      <td>381.621816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>052dd6b36a90b1d68fc38a6151b43caf7cbc9d43.JPG</td>\n",
       "      <td>0.230751</td>\n",
       "      <td>230.750837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05964db76e32bf325ee31238c27cfadab43cabaa.JPG</td>\n",
       "      <td>0.166695</td>\n",
       "      <td>166.695372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image   seconds  milliseconds\n",
       "0  04bda8c273b5ef6b29e1f318a1da3e7506a5e4d8.JPG  0.170163    170.162891\n",
       "1  04e8092d743bef891386f3e0ce82155f12aa4035.JPG  0.169499    169.498501\n",
       "2  04f53ca293e5037fc04f61e49b372d8f003c482d.JPG  0.381622    381.621816\n",
       "3  052dd6b36a90b1d68fc38a6151b43caf7cbc9d43.JPG  0.230751    230.750837\n",
       "4  05964db76e32bf325ee31238c27cfadab43cabaa.JPG  0.166695    166.695372"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = A.Compose([\n",
    "    A.Normalize(mean=MEAN, std=STD),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "def to_tensor(image: np.ndarray) -> torch.Tensor:\n",
    "    return transform(image=image)['image']\n",
    "\n",
    "@torch.no_grad()\n",
    "def timed_inference(tensor: torch.Tensor) -> float:\n",
    "    start = time.perf_counter()\n",
    "    _ = stitcher(tensor)\n",
    "    if DEVICE.type == 'cuda':\n",
    "        torch.cuda.synchronize(DEVICE)\n",
    "    return time.perf_counter() - start\n",
    "\n",
    "# Warmup\n",
    "for path, img in loaded_images[:WARMUP_IMAGES]:\n",
    "    tensor = to_tensor(img)\n",
    "    _ = timed_inference(tensor)\n",
    "\n",
    "records: List[Dict] = []\n",
    "for path, img in loaded_images[WARMUP_IMAGES:]:\n",
    "    tensor = to_tensor(img)\n",
    "    duration = timed_inference(tensor)\n",
    "    records.append({\n",
    "        'image': path.name,\n",
    "        'seconds': duration,\n",
    "        'milliseconds': duration * 1e3,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(records)\n",
    "print(f\"Inferencias medidas: {len(results_df)}\")\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ab0e9",
   "metadata": {},
   "source": [
    "### Métricas resumen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b222dfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media:   193.39 ms\n",
      "Mediana: 171.49 ms\n",
      "p95:     352.77 ms\n",
      "Std:     55.41 ms\n"
     ]
    }
   ],
   "source": [
    "if results_df.empty:\n",
    "    raise RuntimeError('No se midieron inferencias. Revisa configuración.')\n",
    "\n",
    "mean_ms = results_df['milliseconds'].mean()\n",
    "median_ms = results_df['milliseconds'].median()\n",
    "p95_ms = results_df['milliseconds'].quantile(0.95)\n",
    "std_ms = results_df['milliseconds'].std(ddof=1) if len(results_df) > 1 else float('nan')\n",
    "\n",
    "print(f\"Media:   {mean_ms:.2f} ms\")\n",
    "print(f\"Mediana: {median_ms:.2f} ms\")\n",
    "print(f\"p95:     {p95_ms:.2f} ms\")\n",
    "print(f\"Std:     {std_ms:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9450f8b",
   "metadata": {},
   "source": [
    "### Distribución de tiempos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edc811a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.hist(results_df['milliseconds'], bins=15, color='#1982c4', edgecolor='white')\n",
    "ax.set_title('Distribución de tiempos por imagen (ms)')\n",
    "ax.set_xlabel('ms')\n",
    "ax.set_ylabel('conteo')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ff383",
   "metadata": {},
   "source": [
    "### Guardar resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f85c1413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en /mnt/batch/tasks/shared/LS_root/mounts/clusters/asg-lab/code/Users/amir.sadour/proyecto/experiments/inference_rf_detr_small.csv\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_CSV = REPO_ROOT / 'experiments' / 'inference_rf_detr_small.csv'\n",
    "results_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Resultados guardados en {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac1e529-f15b-46fa-8fbb-672a9c0367a9",
   "metadata": {},
   "source": [
    "## HERDNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "911d8b72-4c30-4bea-8027-0111e4811ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from animaloc.models import HerdNet, LossWrapper, load_model\n",
    "from animaloc.eval import HerdNetStitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6282cd8d-9e37-4b82-a8f9-0efa7b5e6809",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CLASSES = {\n",
    "        1: \"Hartebeest\",\n",
    "        2: \"Buffalo\",\n",
    "        3: \"Kob\",\n",
    "        4: \"Warthog\",\n",
    "        5: \"Waterbuck\",\n",
    "        6: \"Elephant\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ba3957f-f8d6-4d23-b642-051d37a15d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = REPO_ROOT / \"outputs/herdnet/best_model_stage_2.pth\"\n",
    "IMAGES_DIR = REPO_ROOT / \"data-delplanque\" / \"test\"  # Directorio de imágenes\n",
    "MAX_IMAGES = 100\n",
    "WARMUP_IMAGES = 5\n",
    "PATCH_SIZE = 512\n",
    "PATCH_OVERLAP = 0\n",
    "BATCH_SIZE = 32\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DOWN_RATIO = 2\n",
    "\n",
    "MEAN = (0.485, 0.456, 0.406)\n",
    "STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "print(f\"Dispositivo: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b16defea-5974-44db-8c19-b9fc7b72ffc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases (incl. background): 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "class_map = checkpoint.get('classes')\n",
    "if class_map:\n",
    "    class_map = {int(k): str(v) for k, v in class_map.items()}\n",
    "else:\n",
    "    class_map = DEFAULT_CLASSES\n",
    "\n",
    "num_classes = len(class_map) + 1  # + background\n",
    "MEAN = tuple(checkpoint.get('mean', MEAN))\n",
    "STD = tuple(checkpoint.get('std', STD))\n",
    "down_ratio = checkpoint.get('down_ratio', DOWN_RATIO)\n",
    "\n",
    "base_model = HerdNet(\n",
    "    num_classes=num_classes,\n",
    "    down_ratio=down_ratio,\n",
    "    num_layers=34,\n",
    "    head_conv=64,\n",
    "    pretrained=False,\n",
    ").to(DEVICE)\n",
    "\n",
    "model = LossWrapper(base_model, losses=[]).to(DEVICE)\n",
    "model = load_model(model, pth_path=str(CHECKPOINT_PATH))\n",
    "model.eval()\n",
    "\n",
    "stitcher = HerdNetStitcher(\n",
    "    model=model,\n",
    "    size=(PATCH_SIZE, PATCH_SIZE),\n",
    "    overlap=PATCH_OVERLAP,\n",
    "    down_ratio=down_ratio,\n",
    "    reduction='mean',\n",
    "    up=False,\n",
    ")\n",
    "\n",
    "print(f\"Clases (incl. background): {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c3aba81-1a61-44e1-a456-ac13ce8b81e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencias medidas: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>seconds</th>\n",
       "      <th>milliseconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04bda8c273b5ef6b29e1f318a1da3e7506a5e4d8.JPG</td>\n",
       "      <td>0.441156</td>\n",
       "      <td>441.156463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04e8092d743bef891386f3e0ce82155f12aa4035.JPG</td>\n",
       "      <td>0.540544</td>\n",
       "      <td>540.544142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04f53ca293e5037fc04f61e49b372d8f003c482d.JPG</td>\n",
       "      <td>0.431285</td>\n",
       "      <td>431.285369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>052dd6b36a90b1d68fc38a6151b43caf7cbc9d43.JPG</td>\n",
       "      <td>0.493922</td>\n",
       "      <td>493.922070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05964db76e32bf325ee31238c27cfadab43cabaa.JPG</td>\n",
       "      <td>0.439957</td>\n",
       "      <td>439.956544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image   seconds  milliseconds\n",
       "0  04bda8c273b5ef6b29e1f318a1da3e7506a5e4d8.JPG  0.441156    441.156463\n",
       "1  04e8092d743bef891386f3e0ce82155f12aa4035.JPG  0.540544    540.544142\n",
       "2  04f53ca293e5037fc04f61e49b372d8f003c482d.JPG  0.431285    431.285369\n",
       "3  052dd6b36a90b1d68fc38a6151b43caf7cbc9d43.JPG  0.493922    493.922070\n",
       "4  05964db76e32bf325ee31238c27cfadab43cabaa.JPG  0.439957    439.956544"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = A.Compose([\n",
    "    A.Normalize(mean=MEAN, std=STD),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "def to_tensor(image: np.ndarray) -> torch.Tensor:\n",
    "    return transform(image=image)['image']\n",
    "\n",
    "@torch.no_grad()\n",
    "def timed_inference(tensor: torch.Tensor) -> float:\n",
    "    start = time.perf_counter()\n",
    "    _ = stitcher(tensor)\n",
    "    if DEVICE.type == 'cuda':\n",
    "        torch.cuda.synchronize(DEVICE)\n",
    "    return time.perf_counter() - start\n",
    "\n",
    "# Warmup\n",
    "for path, img in loaded_images[:WARMUP_IMAGES]:\n",
    "    tensor = to_tensor(img)\n",
    "    _ = timed_inference(tensor)\n",
    "\n",
    "records: List[Dict] = []\n",
    "for path, img in loaded_images[WARMUP_IMAGES:]:\n",
    "    tensor = to_tensor(img)\n",
    "    duration = timed_inference(tensor)\n",
    "    records.append({\n",
    "        'image': path.name,\n",
    "        'seconds': duration,\n",
    "        'milliseconds': duration * 1e3,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(records)\n",
    "print(f\"Inferencias medidas: {len(results_df)}\")\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf951fd6-87b0-4333-a1ba-4ea00ac638bd",
   "metadata": {},
   "source": [
    "### Métricas resumen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ef0b8f8-e432-4c66-8466-f14869a11776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media:   464.01 ms\n",
      "Mediana: 441.27 ms\n",
      "p95:     556.02 ms\n",
      "Std:     50.10 ms\n"
     ]
    }
   ],
   "source": [
    "if results_df.empty:\n",
    "    raise RuntimeError('No se midieron inferencias. Revisa configuración.')\n",
    "\n",
    "mean_ms = results_df['milliseconds'].mean()\n",
    "median_ms = results_df['milliseconds'].median()\n",
    "p95_ms = results_df['milliseconds'].quantile(0.95)\n",
    "std_ms = results_df['milliseconds'].std(ddof=1) if len(results_df) > 1 else float('nan')\n",
    "\n",
    "print(f\"Media:   {mean_ms:.2f} ms\")\n",
    "print(f\"Mediana: {median_ms:.2f} ms\")\n",
    "print(f\"p95:     {p95_ms:.2f} ms\")\n",
    "print(f\"Std:     {std_ms:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2004323a-0cc1-40bd-a8da-d8f79e9b00f0",
   "metadata": {},
   "source": [
    "### Distribución de tiempos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25a0a505-9e94-48ce-b99c-61a1ca05ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.hist(results_df['milliseconds'], bins=15, color='#1982c4', edgecolor='white')\n",
    "ax.set_title('Distribución de tiempos por imagen (ms)')\n",
    "ax.set_xlabel('ms')\n",
    "ax.set_ylabel('conteo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a55cd-38a0-4d21-a4fc-a1c5dfd1127f",
   "metadata": {},
   "source": [
    "### Guardar resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96b372cb-5bbf-4350-9356-05f5d70fe541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en /mnt/batch/tasks/shared/LS_root/mounts/clusters/asg-lab/code/Users/amir.sadour/proyecto/experiments/inference_rf_detr_small.csv\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_CSV = REPO_ROOT / 'experiments' / 'inference_herdnet.csv'\n",
    "results_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Resultados guardados en {OUTPUT_CSV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
